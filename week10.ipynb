{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Biao Feng\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3267: DtypeWarning: Columns (9) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "C:\\Users\\Biao Feng\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:543: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n",
      "C:\\Users\\Biao Feng\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:81: RuntimeWarning: invalid value encountered in log\n",
      "C:\\Users\\Biao Feng\\Anaconda3\\lib\\site-packages\\scipy\\stats\\_distn_infrastructure.py:1904: RuntimeWarning: invalid value encountered in greater\n",
      "  cond1 = (0 < q) & (q < 1)\n",
      "C:\\Users\\Biao Feng\\Anaconda3\\lib\\site-packages\\scipy\\stats\\_distn_infrastructure.py:1904: RuntimeWarning: invalid value encountered in less\n",
      "  cond1 = (0 < q) & (q < 1)\n",
      "C:\\Users\\Biao Feng\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:84: RuntimeWarning: invalid value encountered in log\n",
      "C:\\Users\\Biao Feng\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:90: RuntimeWarning: divide by zero encountered in log\n",
      "C:\\Users\\Biao Feng\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:90: RuntimeWarning: invalid value encountered in log\n",
      "C:\\Users\\Biao Feng\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:93: RuntimeWarning: divide by zero encountered in log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R square score 0.9751975775579671\n",
      "adjusted R squared score 1.0021434192233856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Biao Feng\\Anaconda3\\lib\\site-packages\\scipy\\stats\\_distn_infrastructure.py:1904: RuntimeWarning: invalid value encountered in greater\n",
      "  cond1 = (0 < q) & (q < 1)\n",
      "C:\\Users\\Biao Feng\\Anaconda3\\lib\\site-packages\\scipy\\stats\\_distn_infrastructure.py:1904: RuntimeWarning: invalid value encountered in less\n",
      "  cond1 = (0 < q) & (q < 1)\n",
      "C:\\Users\\Biao Feng\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:90: RuntimeWarning: divide by zero encountered in log\n",
      "C:\\Users\\Biao Feng\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:93: RuntimeWarning: divide by zero encountered in log\n",
      "C:\\Users\\Biao Feng\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:90: RuntimeWarning: invalid value encountered in log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R square score 0.7994575314552362\n",
      "adjusted R squared score -1.286184141410307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Biao Feng\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:81: RuntimeWarning: invalid value encountered in log\n",
      "C:\\Users\\Biao Feng\\Anaconda3\\lib\\site-packages\\scipy\\stats\\_distn_infrastructure.py:1904: RuntimeWarning: invalid value encountered in greater\n",
      "  cond1 = (0 < q) & (q < 1)\n",
      "C:\\Users\\Biao Feng\\Anaconda3\\lib\\site-packages\\scipy\\stats\\_distn_infrastructure.py:1904: RuntimeWarning: invalid value encountered in less\n",
      "  cond1 = (0 < q) & (q < 1)\n",
      "C:\\Users\\Biao Feng\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:84: RuntimeWarning: invalid value encountered in log\n",
      "C:\\Users\\Biao Feng\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:90: RuntimeWarning: divide by zero encountered in log\n",
      "C:\\Users\\Biao Feng\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:93: RuntimeWarning: divide by zero encountered in log\n",
      "C:\\Users\\Biao Feng\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:90: RuntimeWarning: invalid value encountered in log\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 sample(s) (shape=(0, 94)) while a minimum of 1 is required.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-61-3d1fcdbf3742>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    116\u001b[0m                                                 test_size=0.2, random_state=42)\n\u001b[0;32m    117\u001b[0m     \u001b[0mlm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLinearRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 118\u001b[1;33m     \u001b[0mlm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    119\u001b[0m     \u001b[0my_train_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m     \u001b[0my_test_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\base.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    456\u001b[0m         \u001b[0mn_jobs_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    457\u001b[0m         X, y = check_X_y(X, y, accept_sparse=['csr', 'csc', 'coo'],\n\u001b[1;32m--> 458\u001b[1;33m                          y_numeric=True, multi_output=True)\n\u001b[0m\u001b[0;32m    459\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    460\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0matleast_1d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    754\u001b[0m                     \u001b[0mensure_min_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mensure_min_features\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    755\u001b[0m                     \u001b[0mwarn_on_dtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwarn_on_dtype\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 756\u001b[1;33m                     estimator=estimator)\n\u001b[0m\u001b[0;32m    757\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    758\u001b[0m         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    580\u001b[0m                              \u001b[1;34m\" minimum of %d is required%s.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    581\u001b[0m                              % (n_samples, shape_repr, ensure_min_samples,\n\u001b[1;32m--> 582\u001b[1;33m                                 context))\n\u001b[0m\u001b[0;32m    583\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    584\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mensure_min_features\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Found array with 0 sample(s) (shape=(0, 94)) while a minimum of 1 is required."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from DataLoader import DataLoader as DL\n",
    "\n",
    "import statsmodels.tsa.api as sta\n",
    "import statsmodels.tsa.stattools as stattools\n",
    "import statsmodels.tsa.api as smt\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.stats.stattools as stools\n",
    "\n",
    "import scipy.stats as stats\n",
    "## Harsh code for regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def norm_rank(kk):\n",
    "    ## for every row, rank from small to big, then minus 0.5, \n",
    "    ## then divide number of data in row\n",
    "    ## ppf: inverse function of cdf\n",
    "    ## - 0.5 make sure no 1 or 0 in the ppf\n",
    "    tmp = stats.norm.ppf((kk.rank(1)-.5).div(kk.count(1), 0))\n",
    "    ## set index and columns\n",
    "    kk = pd.DataFrame(tmp, index=kk.index, columns=kk.columns)\n",
    "    ## normalization\n",
    "    kk = kk.div(kk.std(1,ddof=0), 0)\n",
    "    return kk\n",
    "\n",
    "def get_cross_section(df,col_name):\n",
    "    tt = df.stack().reset_index().rename(columns={0: col_name})\n",
    "    return tt\n",
    "\n",
    "def get_independent_variable(df):\n",
    "    \n",
    "    tt = np.log(df.loc[st:,cols].rolling(20).mean())\n",
    "    tt = norm_rank(tt).stack().reset_index().rename(columns={0: 'col_name'})\n",
    "    \n",
    "    return tt\n",
    "\n",
    "\n",
    "loc='C:/Users/Biao Feng/source/Anaconda3/practicum/'\n",
    "d = DL(loc)\n",
    "\n",
    "##load all fundimental variables\n",
    "for i in range(0,len(d.cols_q)):\n",
    "    globals()[d.cols_q[i]] = d.get_time_series(d.merged_table, d.cols_q[i], icol='datadate')\n",
    "\n",
    "mcap = d.get_time_series(d.merged_table, 'mcap')\n",
    "ret = d.get_time_series(d.merged_table, 'RET')\n",
    "\n",
    "## if data's mcap is bigger than breakpoint\n",
    "d.merged_table.loc[:, 'in_universe']=[np.nan if pd.isnull(m) or pd.isnull(b) or m < b else 1 \n",
    "                                      for m,b in zip(d.merged_table.mcap.values, d.merged_table.breakpt.values)]\n",
    "\n",
    "# use data only after 1977\n",
    "st = '1977'\n",
    "\n",
    "industry = ['i1','i2','i3','i4','i5','i6','i7','i8']\n",
    "adjusted_score = []\n",
    "def adj_r2_score(model,y,yhat):\n",
    "    adj = 1 - float(len(y)-1)/(len(y)-len(model.coef_)-1)*(1 - r2_score(y,yhat))\n",
    "    return adj\n",
    "    \n",
    "#make example by industry 11\n",
    "#for industry in industry:\n",
    "industry = 'i1'\n",
    "#get industry universe\n",
    "in_universe = d.get_time_series(d.merged_table[d.merged_table[industry]==1], 'in_universe')\n",
    "in_univ = in_universe[st:].fillna(method='ffill', limit=3)\n",
    "cols = in_univ.count()[in_univ.count()>20].index\n",
    "\n",
    "##dependent variable\n",
    "# book value growth\n",
    "equity = (atq.loc[st:,cols]-ltq.loc[st:,cols]).rolling(20).mean()\n",
    "equity_chg = np.log(equity).diff()\n",
    "y_equchg = norm_rank(equity_chg).stack().reset_index().rename(columns={0: 'book_value growth'})\n",
    "# book to market\n",
    "b2m = np.log(equity / mcap * 1000)\n",
    "y_b2m = norm_rank(b2m).stack().reset_index().rename(columns={0: 'book to market'})\n",
    "\n",
    "##independent variable\n",
    "for i in range(0,len(d.cols_q)):\n",
    "    # 5 year mean\n",
    "    tt = np.log(globals()[d.cols_q[i]].loc[st:,cols].rolling(20).mean())\n",
    "    globals()['x_mean_' + d.cols_q[i]] = norm_rank(tt).stack().reset_index().rename(columns={0: d.cols_q[i]+'_mean'})\n",
    "    # 5 year std\n",
    "    ff = np.log(globals()[d.cols_q[i]].loc[st:,cols].rolling(20).std())\n",
    "    globals()['x_std_' + d.cols_q[i]] = norm_rank(ff).stack().reset_index().rename(columns={0: d.cols_q[i]+'_std'})\n",
    "\n",
    "## get regression penal\n",
    "## use method 'left' have NaN, cannot regress, so use 'inner', but too less row\n",
    "reg_penal = pd.merge(y_equchg,y_b2m,on=['gvkey', 'datadate'], how ='inner')\n",
    "for i in range(0,len(d.cols_q)):\n",
    "    ##  threshold for independent variable, if data number is less than it, \n",
    "    ##  do not merge in the reg_penal\n",
    "    ## threshold should be changed depend on different industry\n",
    "    if len(globals()['x_mean_' + d.cols_q[i]]) > 900:\n",
    "        reg_penal = pd.merge(reg_penal, globals()['x_mean_' + d.cols_q[i]],\n",
    "                             on=['gvkey', 'datadate'], how ='inner')\n",
    "        reg_penal = pd.merge(reg_penal, globals()['x_std_' + d.cols_q[i]],\n",
    "                             on=['gvkey', 'datadate'], how ='inner')\n",
    "\n",
    "#####################################################################################\n",
    "## regression from now\n",
    "\n",
    "X = reg_penal.iloc[:,4:].values\n",
    "y = reg_penal['book_value growth'].values        \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                            test_size=0.2, random_state=42)\n",
    "lm = LinearRegression()\n",
    "lm.fit(X_train,y_train)\n",
    "y_train_pred = lm.predict(X_train)\n",
    "y_test_pred = lm.predict(X_test)\n",
    "print('R square score',r2_score(y_test, y_test_pred))\n",
    "print('adjusted R squared score',adj_r2_score(lm, y_test,y_test_pred))\n",
    "    #adjusted_score.append(adj_r2_score(lm, y_test,y_test_pred))\n",
    "#adjusted = pd.DataFrame([adjusted_score], columns = industry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "X = reg_penal.iloc[:,4:].values\n",
    "y = reg_penal['book_value growth'].values        \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                            test_size=0.2, random_state=42)\n",
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:/Users/hecha/Downloads/practicum/main_work/link table 1962-2001_csv.zip'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-60-fb06961a3398>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[0mloc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'C:/Users/hecha/Downloads/practicum/main_work/'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m \u001b[0md\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDL\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;31m##load all fundimental variables\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\source\\Anaconda3\\practicum\\DataLoader.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, loc)\u001b[0m\n\u001b[0;32m     22\u001b[0m                     ]\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgood\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\source\\Anaconda3\\practicum\\DataLoader.py\u001b[0m in \u001b[0;36mload_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     34\u001b[0m                                  \u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m                                  \u001b[0mparse_dates\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'datadate'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m                                  usecols=['datadate', 'GVKEY', 'LPERMNO', 'conm'])\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m         crsp = pd.read_csv('{loc}/crsp returns all 1962-2001_csv.zip'.format(loc=self.data_location),\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 678\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    438\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    439\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 440\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    441\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    785\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    786\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 787\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    788\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    789\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1012\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1013\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1014\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1015\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1016\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1706\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'usecols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1707\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1708\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1709\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1710\u001b[0m         \u001b[0mpassed_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnames\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\zipfile.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, file, mode, compression, allowZip64, compresslevel)\u001b[0m\n\u001b[0;32m   1202\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1203\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1204\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilemode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1205\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1206\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mfilemode\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodeDict\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:/Users/hecha/Downloads/practicum/main_work/link table 1962-2001_csv.zip'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from DataLoader import DataLoader as DL\n",
    "\n",
    "import statsmodels.tsa.api as sta\n",
    "import statsmodels.tsa.stattools as stattools\n",
    "import statsmodels.tsa.api as smt\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.stats.stattools as stools\n",
    "\n",
    "import scipy.stats as stats\n",
    "\n",
    "def norm_rank(kk):\n",
    "    ## for every row, rank from small to big, then minus 0.5, \n",
    "    ## then divide number of data in row\n",
    "    ## ppf: inverse function of cdf\n",
    "    ## - 0.5 make sure no 1 or 0 in the ppf\n",
    "    tmp = stats.norm.ppf((kk.rank(1)-.5).div(kk.count(1), 0))\n",
    "    ## set index and columns\n",
    "    kk = pd.DataFrame(tmp, index=kk.index, columns=kk.columns)\n",
    "    ## normalization\n",
    "    kk = kk.div(kk.std(1,ddof=0), 0)\n",
    "    return kk\n",
    "\n",
    "def get_cross_section(df,col_name):\n",
    "    tt = df.stack().reset_index().rename(columns={0: col_name})\n",
    "    return tt\n",
    "\n",
    "def get_independent_variable(df):\n",
    "    \n",
    "    tt = np.log(df.loc[st:,cols].rolling(20).mean())\n",
    "    tt = norm_rank(tt).stack().reset_index().rename(columns={0: 'col_name'})\n",
    "    \n",
    "    return tt\n",
    "\n",
    "\n",
    "loc='C:/Users/hecha/Downloads/practicum/main_work/'\n",
    "d = DL(loc)\n",
    "\n",
    "##load all fundimental variables\n",
    "for i in range(0,len(d.cols_q)):\n",
    "    globals()[d.cols_q[i]] = d.get_time_series(d.merged_table, d.cols_q[i], icol='datadate')\n",
    "\n",
    "mcap = d.get_time_series(d.merged_table, 'mcap')\n",
    "ret = d.get_time_series(d.merged_table, 'RET')\n",
    "\n",
    "## if data's mcap is bigger than breakpoint\n",
    "d.merged_table.loc[:, 'in_universe']=[np.nan if pd.isnull(m) or pd.isnull(b) or m < b else 1 \n",
    "                                      for m,b in zip(d.merged_table.mcap.values, d.merged_table.breakpt.values)]\n",
    "\n",
    "# use data only after 1977\n",
    "st = '1977'\n",
    "\n",
    "#make example by industry 11\n",
    "industry = 'i2'\n",
    "\n",
    "#get industry universe\n",
    "in_universe = d.get_time_series(d.merged_table[d.merged_table[industry]==1], 'in_universe')\n",
    "in_univ = in_universe[st:].fillna(method='ffill', limit=3)\n",
    "cols = in_univ.count()[in_univ.count()>20].index\n",
    "\n",
    "##dependent variable\n",
    "# book value growth\n",
    "equity = (atq.loc[st:,cols]-ltq.loc[st:,cols]).rolling(20).mean()\n",
    "equity_chg = np.log(equity).diff()\n",
    "y_equchg = norm_rank(equity_chg).stack().reset_index().rename(columns={0: 'book_value growth'})\n",
    "# book to market\n",
    "b2m = np.log(equity / mcap * 1000)\n",
    "y_b2m = norm_rank(b2m).stack().reset_index().rename(columns={0: 'book to market'})\n",
    "\n",
    "##independent variable\n",
    "for i in range(0,len(d.cols_q)):\n",
    "    # 5 year mean\n",
    "    tt = np.log(globals()[d.cols_q[i]].loc[st:,cols].rolling(20).mean())\n",
    "    globals()['x_mean_' + d.cols_q[i]] = norm_rank(tt).stack().reset_index().rename(columns={0: d.cols_q[i]+'_mean'})\n",
    "    # 5 year std\n",
    "    ff = np.log(globals()[d.cols_q[i]].loc[st:,cols].rolling(20).std())\n",
    "    globals()['x_std_' + d.cols_q[i]] = norm_rank(ff).stack().reset_index().rename(columns={0: d.cols_q[i]+'_std'})\n",
    "\n",
    "## get regression penal\n",
    "## use method 'left' have NaN, cannot regress, so use 'inner', but too less row\n",
    "reg_penal = pd.merge(y_equchg,y_b2m,on=['gvkey', 'datadate'], how ='inner')\n",
    "for i in range(0,len(d.cols_q)):\n",
    "    ##  threshold for independent variable, if data number is less than it, \n",
    "    ##  do not merge in the reg_penal\n",
    "    ## threshold should be changed depend on different industry\n",
    "    if len(globals()['x_mean_' + d.cols_q[i]]) > 900:\n",
    "        reg_penal = pd.merge(reg_penal, globals()['x_mean_' + d.cols_q[i]],\n",
    "                             on=['gvkey', 'datadate'], how ='inner')\n",
    "        reg_penal = pd.merge(reg_penal, globals()['x_std_' + d.cols_q[i]],\n",
    "                             on=['gvkey', 'datadate'], how ='inner')\n",
    "\n",
    "\n",
    "\n",
    "#####################################################################################\n",
    "## regression from now\n",
    "\n",
    "## Harsh code for regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "X = reg_penal.iloc[:,4:].values\n",
    "y = reg_penal['book_value growth'].values        \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                            test_size=0.4, random_state=101)\n",
    "\n",
    "lm = LinearRegression()\n",
    "lm.fit(X_train,y_train)\n",
    "\n",
    "y_train_pred = lm.predict(X_train)\n",
    "y_test_pred = lm.predict(X_test)\n",
    "\n",
    "plt.scatter(y_test,y_test_pred)\n",
    "\n",
    "print('MSE train: %.3f, test: %.3f' % (\n",
    "        mean_squared_error(y_train, y_train_pred),\n",
    "        mean_squared_error(y_test, y_test_pred)))\n",
    "print('R^2 train: %.3f, test: %.3f' % (\n",
    "        r2_score(y_train, y_train_pred),\n",
    "        r2_score(y_test, y_test_pred)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
